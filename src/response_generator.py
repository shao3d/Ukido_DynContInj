from pathlib import Path
from typing import List, Dict, Optional
from config import Config
from openrouter_client import OpenRouterClient
from standard_responses import DEFAULT_FALLBACK
from offers_catalog import get_offer, get_tone_adaptation, get_dynamic_example
import re

class ResponseGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–∞:
    - –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–æ—É—Ç–µ—Ä–∞ (status=success, documents, decomposed_questions)
    - –ü–æ–¥–≥—Ä—É–∂–∞–µ—Ç MD –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ data/documents_compressed (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)
    - –°–æ–±–∏—Ä–∞–µ—Ç —Å–æ—Å—Ç–∞–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç (—Å–∏—Å—Ç–µ–º–Ω–∞—è —Ä–æ–ª—å + –¥–æ–∫—É–º–µ–Ω—Ç—ã + –∏—Å—Ç–æ—Ä–∏—è[–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10] + –≤–æ–ø—Ä–æ—Å—ã)
    - –í—ã–∑—ã–≤–∞–µ—Ç LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """

    def __init__(self, docs_dir: Optional[Path] = None):
        cfg = Config()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude 3.5 Haiku –¥–ª—è –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Å—Ç–∏–ª–µ–º
        self.client = OpenRouterClient(
            cfg.OPENROUTER_API_KEY,
            seed=cfg.SEED,
            max_tokens=550,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏
            temperature=0.1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            model="anthropic/claude-3.5-haiku",  # Claude Haiku –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        )
        self.docs_dir = docs_dir or (Path(__file__).parent.parent / "data" / "documents_compressed")
        self.history_limit = cfg.HISTORY_LIMIT  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞

    async def generate(
        self,
        router_result: Dict,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        if router_result.get("status") != "success":
            return DEFAULT_FALLBACK

        docs = router_result.get("documents") or []
        questions = router_result.get("decomposed_questions") or []
        if not docs or not questions:
            return DEFAULT_FALLBACK

        # –ü–æ–ª—É—á–∞–µ–º user_signal –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        user_signal = router_result.get("user_signal", "exploring_only")
        
        doc_texts = self._load_docs(docs)
        
        # –û–¥–Ω–æ—ç—Ç–∞–ø–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å Claude Haiku + dynamic few-shot
        messages = self._build_messages(doc_texts, questions, history or [], router_result)

        try:
            reply = await self.client.chat(messages)
            cleaned = (reply or "").strip()
            if not cleaned:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
            
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            sanitized = self._strip_source_citations(cleaned)
            polished = self._remove_question_headings(sanitized)
            humanized = self._humanize_missing_info(polished)
            no_labels = self._strip_service_labels(humanized)
            no_cta = self._strip_generic_cta(no_labels)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è (—É–±–∏—Ä–∞–µ–º –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)
            final_text = self._final_sanitize(no_cta)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –∫–æ–Ω–µ—Ü (–µ—Å–ª–∏ –µ—Å—Ç—å)
            offer = get_offer(user_signal)
            if offer and offer["priority"] in ["high", "medium"]:
                final_text = self._inject_offer(final_text, offer)
            
            return final_text
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –Ω–µ–ø–æ–ª–∞–¥–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."

    def _load_docs(self, docs: List[str]) -> Dict[str, str]:
        """–ß–∏—Ç–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ü–µ–ª–∏–∫–æ–º –∏–∑ data/documents_compressed.
        –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≤—ã–±—Ä–∞–Ω–Ω—ã–µ Router (–¥–æ 4 —à—Ç—É–∫)."""
        texts: Dict[str, str] = {}
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è - —É–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—ã
        unique_docs = list(dict.fromkeys(docs))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (Router —É–∂–µ –æ–≥—Ä–∞–Ω–∏—á–∏–ª –¥–æ 4)
        docs_to_load = unique_docs
        
        for name in docs_to_load:
            try:
                path = self.docs_dir / name
                content = path.read_text(encoding="utf-8")
                texts[name] = content
            except FileNotFoundError:
                print(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {name} ({self.docs_dir})")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {name}: {e}")
        return texts

    def _build_messages(
        self,
        doc_texts: Dict[str, str],
        questions: List[str],
        history: List[Dict[str, str]],
        router_result: Dict,
    ) -> List[Dict[str, str]]:
        # –ü–æ–ª—É—á–∞–µ–º user_signal –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ç–æ–Ω–∞
        user_signal = router_result.get("user_signal", "exploring_only")
        tone_adaptation = get_tone_adaptation(user_signal)
        dynamic_example = get_dynamic_example(user_signal)
        
        # –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Claude Haiku - —Ñ–∞–∫—Ç—ã + —Å—Ç–∏–ª—å + –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        system_role = (
            "–¢—ã ‚Äî –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¥–µ—Ç—Å–∫–æ–π —à–∫–æ–ª—ã soft skills Ukido. "
            "–û—Ç–≤–µ—á–∞–π –∂–∏–≤—ã–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º –æ—Ç –ª–∏—Ü–∞ —à–∫–æ–ª—ã (–∏—Å–ø–æ–ª—å–∑—É–π '–º—ã', –Ω–µ '—è'). "
            "–ì–æ–≤–æ—Ä–∏ –∫–∞–∫ –±—É–¥—Ç–æ –∫–æ–ª–ª–µ–∫—Ç–∏–≤ —à–∫–æ–ª—ã —Å–æ–≤–µ—Ç—É–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª—é. "
            "–ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. "
            "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö ‚Äî —Å–∫–∞–∂–∏: '–í –Ω–∞—à–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —ç—Ç–æ–≥–æ –Ω–µ—Ç.' "
            "–í—Å–µ —Ñ–∞–∫—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Ü–µ–Ω—ã ‚Äî –¢–û–õ–¨–ö–û –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏—é —Ç–æ–Ω–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if tone_adaptation.get("style"):
            system_role += f"\n\n–ê–î–ê–ü–¢–ê–¶–ò–Ø –¢–û–ù–ê:\n{tone_adaptation['style']}"
            # –í–ê–ñ–ù–û: –£—Å–∏–ª–∏–≤–∞–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–Ω–∞ –¥–ª—è user_signal
            tone_map = {
                "price_sensitive": "üéØ –ö–†–ò–¢–ò–ß–ù–û: –†–æ–¥–∏—Ç–µ–ª—å —Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ü–µ–Ω–µ! –í –ö–ê–ñ–î–û–ú –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≤—ã–≥–æ–¥—É, ROI, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ú–µ–Ω—å—à–µ —ç–º–ø–∞—Ç–∏–∏ - –±–æ–ª—å—à–µ —Ñ–∞–∫—Ç–æ–≤ –æ —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –ì–æ–≤–æ—Ä–∏ –æ –¥–µ–Ω—å–≥–∞—Ö –ø—Ä—è–º–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ.",
                "anxiety_about_child": "–í–°–ï–ì–î–ê –Ω–∞—á–∏–Ω–∞–π —Å —ç–º–ø–∞—Ç–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è",
                "ready_to_buy": "–í–°–ï–ì–î–ê –¥–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–µ–∑ –≤–æ–¥—ã",
                "exploring_only": "–í–°–ï–ì–î–ê –±—É–¥—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º"
            }
            if user_signal in tone_map:
                system_role += f"\n\n{tone_map[user_signal]}"

        # –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        allowed_docs = list(doc_texts.keys())

        # –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ —Ç—Ä–∏–º–º–∏–Ω–≥–∞)
        docs_block_lines = []
        for name, text in doc_texts.items():
            docs_block_lines.append(f"=== –î–æ–∫—É–º–µ–Ω—Ç: {name} ===\n{text}\n")
        docs_block = "\n".join(docs_block_lines) if docs_block_lines else "=== –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ==="

        system_content = (
            f"{system_role}\n\n"
            f"–†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(allowed_docs) if allowed_docs else '‚Äî'}\n\n"
            f"=== –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π ===\n{docs_block}\n\n"
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        if dynamic_example:
            system_content += f"=== –ü–†–ò–ú–ï–† –ê–î–ê–ü–¢–ê–¶–ò–ò –°–¢–ò–õ–Ø ===\n{dynamic_example}\n\n"
        
        system_content += (
            "–°—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–∞:\n"
            "‚Ä¢ –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —è–∑—ã–∫ –æ—Ç –ª–∏—Ü–∞ —à–∫–æ–ª—ã (–∏—Å–ø–æ–ª—å–∑—É–π '–º—ã', '—É –Ω–∞—Å', '–Ω–∞—à–∏')\n"
            "‚Ä¢ –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤–º–µ—Å—Ç–æ –∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏—Ö\n"
            "‚Ä¢ 100-150 —Å–ª–æ–≤, –Ω–µ –±–æ–ª—å—à–µ\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:\n"
            "‚Ä¢ –ü–ï–†–í–´–ï 2 –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!\n"
            "‚Ä¢ –ù–∞—á–∏–Ω–∞–π —Å –≥–ª–∞–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∞ –Ω–µ —Å –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤\n"
            "‚Ä¢ –ï—Å–ª–∏ —Ä–æ–¥–∏—Ç–µ–ª—å —Ä–∞–∑–¥—Ä–∞–∂—ë–Ω - –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ\n"
            "‚Ä¢ –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ —Ü–µ–Ω–µ - —Å—Ä–∞–∑—É –Ω–∞–∑—ã–≤–∞–π —Ü–µ–Ω—É, –ø–æ—Ç–æ–º –æ–±—ä—è—Å–Ω—è–π —Ü–µ–Ω–Ω–æ—Å—Ç—å\n"
            "‚Ä¢ –î–µ—Ç–∞–ª–∏ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è - —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n\n"
            "–ü—Ä–∏–º–µ—Ä—ã —Ö–æ—Ä–æ—à–∏—Ö –Ω–∞—á–∞–ª:\n"
            "‚Ä¢ –í–æ–ø—Ä–æ—Å –æ —Ü–µ–Ω–µ ‚Üí '–ú–µ—Å—è—Ü –æ–±—É—á–µ–Ω–∏—è —Å—Ç–æ–∏—Ç 6,000 –≥—Ä–Ω. –ó–∞ —ç—Ç–∏ –¥–µ–Ω—å–≥–∏ —Ä–µ–±—ë–Ω–æ–∫ –ø–æ–ª—É—á–∞–µ—Ç...'\n"
            "‚Ä¢ –°–æ–º–Ω–µ–Ω–∏—è –≤ –ø–æ–ª—å–∑–µ ‚Üí '–î–µ—Ç–∏ —É—á–∞—Ç—Å—è –≤—ã—Å—Ç—É–ø–∞—Ç—å –ø—É–±–ª–∏—á–Ω–æ –∏ —É–ø—Ä–∞–≤–ª—è—Ç—å —ç–º–æ—Ü–∏—è–º–∏. –ü–æ—Å–ª–µ –∫—É—Ä—Å–∞ 85% —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —É–≤–µ—Ä–µ–Ω–Ω–µ–µ...'\n"
            "‚Ä¢ –†–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ ‚Üí '–ü–æ–Ω–∏–º–∞–µ–º –≤–∞—à–∏ –æ–ø–∞—Å–µ–Ω–∏—è –æ —Ü–µ–Ω–µ. –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º —á—Ç–æ –≤—Ö–æ–¥–∏—Ç –≤ —Å—Ç–æ–∏–º–æ—Å—Ç—å...'\n\n"
            "–§–∞–∫—Ç—ã –∏ —Ü–∏—Ñ—Ä—ã:\n"
            "‚Ä¢ –í—Å–µ —Ü–∏—Ñ—Ä—ã, —Ü–µ–Ω—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã - –¢–û–õ–¨–ö–û –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ç–æ—á–Ω–æ\n"
            "‚Ä¢ –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –æ—Ç —Å–µ–±—è\n"
            "‚Ä¢ –ï—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ—Ç - —Å–∫–∞–∂–∏ '–í –Ω–∞—à–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —ç—Ç–æ–≥–æ –Ω–µ—Ç'\n\n"
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤:\n"
            "‚Ä¢ –¢—ã –≤–∏–¥–∏—à—å –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Å–æ–æ–±—â–µ–Ω–∏–π\n"
            "‚Ä¢ –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π —Ç—ã —É–∂–µ –æ—Ç–≤–µ—á–∞–ª, –∏–ª–∏ –ø—Ä–æ—Å–∏—Ç –ø–æ–≤—Ç–æ—Ä–∏—Ç—å/—É—Ç–æ—á–Ω–∏—Ç—å:\n"
            "  - –í–µ–∂–ª–∏–≤–æ –Ω–∞–ø–æ–º–Ω–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è: '–ö–∞–∫ —è —É–ø–æ–º–∏–Ω–∞–ª...', '–ù–∞–ø–æ–º–Ω—é, —á—Ç–æ...', '–î–∞, –µ—â–µ —Ä–∞–∑ - ...'\n"
            "  - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –≥—Ä—É–±—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Ç–∏–ø–∞ '–≤—ã —É–∂–µ —Å–ø—Ä–∞—à–∏–≤–∞–ª–∏' –∏–ª–∏ '—è —É–∂–µ –æ—Ç–≤–µ—á–∞–ª'\n"
            "‚Ä¢ –ê–¥–∞–ø—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫ –ø—Ä–∏—á–∏–Ω–µ –ø–æ–≤—Ç–æ—Ä–∞ (–∑–∞–±—ã–ª/–Ω–µ –ø–æ–Ω—è–ª/—É—Ç–æ—á–Ω—è–µ—Ç)\n\n"
            "–ò–∑–±–µ–≥–∞–π:\n"
            "‚Ä¢ –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤\n"
            "‚Ä¢ –ö–ª–∏—à–µ: '–ó–Ω–∞–µ—Ç–µ', '–ú–Ω–æ–≥–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–∏ –æ—Ç–º–µ—á–∞—é—Ç', '—Ç–∞–∫ —á—Ç–æ', '–Ω–µ –ø–µ—Ä–µ–∂–∏–≤–∞–π—Ç–µ'\n"
            "‚Ä¢ –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫: '–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è', '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è'\n"
            "‚Ä¢ –ü–æ–≤—Ç–æ—Ä–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
            "‚Ä¢ –ù–∞–≤—è–∑—á–∏–≤—ã—Ö CTA –≤ –∫–æ–Ω—Ü–µ ('–ü–∏—à–∏—Ç–µ', '–ó–≤–æ–Ω–∏—Ç–µ', '–û—Å—Ç–∞–ª–∏—Å—å –≤–æ–ø—Ä–æ—Å—ã?')\n"
            "‚Ä¢ –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π ('–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–ü—Ä–∏–≤–µ—Ç') –µ—Å–ª–∏ –¥–∏–∞–ª–æ–≥ —É–∂–µ –Ω–∞—á–∞–ª—Å—è - —Å—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—Ç–∏\n\n"
            "–ü—Ä–∏–º–µ—Ä—ã –∑–∞–º–µ–Ω:\n"
            "‚Ä¢ '–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è' ‚Üí '–¥–µ–ª–∞–µ–º'\n"
            "‚Ä¢ '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å' ‚Üí '–º–æ–∂–Ω–æ'\n"
            "‚Ä¢ '–Ω–∞—à–∏ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏' ‚Üí '–Ω–∞—à–∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏'\n"
        )

        messages: List[Dict[str, str]] = [{"role": "system", "content": system_content}]

        # –ò—Å—Ç–æ—Ä–∏—è: —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
        trimmed_history = history[-self.history_limit :] if len(history) > self.history_limit else history
        if trimmed_history:
            messages.extend(trimmed_history)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
        social_context = router_result.get("social_context")
        social_instruction = ""
        if social_context:
            # Router —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π social_context (greeting –∏–ª–∏ repeated_greeting)
            # –ë–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏
            social_map = {
                "greeting": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–∑–¥–æ—Ä–æ–≤–∞–ª—Å—è. –ù–∞—á–Ω–∏ –æ—Ç–≤–µ—Ç —Å –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è.",
                "repeated_greeting": "–í–ê–ñ–ù–û: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–¥–æ—Ä–æ–≤–∞–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ. –ù–ï –∑–¥–æ—Ä–æ–≤–∞–π—Å—è —Å–Ω–æ–≤–∞! –ú–æ–∂–µ—à—å –º—è–≥–∫–æ –æ—Ç–º–µ—Ç–∏—Ç—å —ç—Ç–æ ('–ú—ã —É–∂–µ –ø–æ–∑–¥–æ—Ä–æ–≤–∞–ª–∏—Å—å :)' –∏–ª–∏ '–ï—â–µ —Ä–∞–∑ –∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!'), –∑–∞—Ç–µ–º —Å—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–π –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞.",
                "thanks": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª. –ù–∞—á–Ω–∏ —Å –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–æ–º–æ—á—å.",
                "apology": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–∑–≤–∏–Ω–∏–ª—Å—è. –ù–∞—á–Ω–∏ —Å —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â–µ–π —Ñ—Ä–∞–∑—ã, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–µ–π —á—Ç–æ –≤—Å—ë —Ö–æ—Ä–æ—à–æ –∏ –Ω–µ —Å—Ç–æ–∏—Ç –±–µ—Å–ø–æ–∫–æ–∏—Ç—å—Å—è.",
                "farewell": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—â–∞–µ—Ç—Å—è. –î–æ–±–∞–≤—å –ø—Ä–æ—â–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞."
            }
            social_instruction = social_map.get(social_context, "") + "\n"
        
        questions_block = "\n".join(f"- {q}" for q in questions)
        messages.append(
            {
                "role": "user",
                "content": (
                    social_instruction +
                    "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º. "
                    "–û–±—ä—ë–º 100-150 —Å–ª–æ–≤. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π —Ç–æ, —á—Ç–æ —É–∂–µ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ.\n"
                    "–ê—Å–ø–µ–∫—Ç—ã –¥–ª—è —É—á—ë—Ç–∞:\n" + questions_block
                ),
            }
        )
        return messages

    def _strip_source_citations(self, text: str) -> str:
        """–ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ—Ç –º–µ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤–∏–¥–∞ [doc: filename.md] –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞."""
        pattern = re.compile(r"\[doc:\s*[^\]]+\]")
        return pattern.sub("", text)

    # --- –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ---
    def _remove_question_headings(self, text: str) -> str:
        """–£–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏-–∑–∞–≥–æ–ª–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥—É–±–ª–∏—Ä—É—é—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã,
        –≤–∏–¥–∞ '1. **...?...**' –∏–ª–∏ '- **...?...**' –≤ –Ω–∞—á–∞–ª–µ –±–ª–æ–∫–æ–≤. –ù–µ —Ç—Ä–æ–≥–∞–µ—Ç –æ–±—ã—á–Ω—ã–µ —Å–ø–∏—Å–∫–∏.
        """
        lines = text.splitlines()
        cleaned_lines: List[str] = []
        heading_re = re.compile(r"^\s*(?:\d+\.|-)\s*\*\*[^*\n]*\?\*\*\s*$")
        for ln in lines:
            if heading_re.match(ln):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–∞–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                continue
            cleaned_lines.append(ln)
        # –°–∂–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        out: List[str] = []
        empty = 0
        for ln in cleaned_lines:
            if ln.strip() == "":
                empty += 1
                if empty <= 2:
                    out.append(ln)
            else:
                empty = 0
                out.append(ln)
        return "\n".join(out).strip()

    def _humanize_missing_info(self, text: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç —Å—É—Ö–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª–µ–µ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–µ.
        –ü—Ä–∏–º–µ—Ä—ã: "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö", "–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ —É–∫–∞–∑–∞–Ω–æ" ‚Üí —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è —Ñ—Ä–∞–∑–∞.
        """
        replacements = [
            r"–Ω–µ—Ç\s+–¥–∞–Ω–Ω—ã—Ö\s+–≤\s+–¥–æ–∫—É–º–µ–Ω—Ç(–∞—Ö|–∞—Ö–∞—Ö|–∞—Ö–∞—Ö)?",
            r"–≤\s+–¥–æ–∫—É–º–µ–Ω—Ç(–∞—Ö|–∞—Ö–∞—Ö|–∞—Ö–∞—Ö)?\s+–Ω–µ\s+—É–∫–∞–∑–∞–Ω–æ",
            r"–∏–Ω—Ñ–æ—Ä–º–∞—Ü(–∏–∏|–∏—è)\s+–≤\s+–¥–æ–∫—É–º–µ–Ω—Ç(–∞—Ö|–∞—Ö–∞—Ö|–∞—Ö–∞—Ö)?\s+–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
        ]
        friendly = "–í –Ω–∞—à–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —ç—Ç–æ–≥–æ –Ω–µ—Ç."
        out = text
        for pat in replacements:
            out = re.sub(pat, friendly, out, flags=re.IGNORECASE)
        return out

    def _strip_service_labels(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–∏–¥–∞ '–ö–æ—Ä–æ—Ç–∫–æ:', '–í–∞–∂–Ω–æ:', '–ò—Ç–æ–≥–æ:', '–ú–æ–≥—É –ø–æ–º–æ—á—å:'
        –ü—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ –¥–≤–æ–µ—Ç–æ—á–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)."""
        out_lines: List[str] = []
        label_re = re.compile(r"^\s*(–ö–æ—Ä–æ—Ç–∫–æ|–í–∞–∂–Ω–æ|–ò—Ç–æ–≥–æ|–ú–æ–≥—É –ø–æ–º–æ—á—å)\s*:\s*(.*)$", re.IGNORECASE)
        for ln in text.splitlines():
            m = label_re.match(ln)
            if m:
                content_after = m.group(2).strip()
                if content_after:
                    out_lines.append(content_after)
                # –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –ª–µ–π–±–ª –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É
            else:
                out_lines.append(ln)
        return "\n".join(out_lines)

    def _strip_generic_cta(self, text: str) -> str:
        """–£–±–∏—Ä–∞–µ—Ç –Ω–∞–≤—è–∑—á–∏–≤—ã–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ CTA –≤—Ä–æ–¥–µ '–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã...' –∏ –ø–æ—Ö–æ–∂–∏–µ."""
        patterns = [
            r"^\s*–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å .*–≤–æ–ø—Ä–æ—Å",
            r"^\s*–ï—Å–ª–∏ –±—É–¥—É—Ç –≤–æ–ø—Ä–æ—Å—ã",
            r"^\s*–ì–æ—Ç–æ–≤(–∞|—ã)? –ø–æ–º–æ—á—å",
            r"^\s*–ú–æ–≥—É —É—Ç–æ—á–Ω–∏—Ç—å —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞",
            r"^\s*–Ø –º–æ–≥—É .* (—É—Ç–æ—á–Ω–∏—Ç—å|–ø–æ–º–æ—á—å)",
        ]
        lines = [ln for ln in text.splitlines() if not any(re.search(p, ln, flags=re.IGNORECASE) for p in patterns)]
        # —Ç–∞–∫–∂–µ —É–¥–∞–ª–∏–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–µ
        while lines and lines[-1].strip() == "":
            lines.pop()
        return "\n".join(lines)

    # –£–¥–∞–ª—è–µ–º –º–µ—Ç–æ–¥ _stylize_response, —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
    
    def _final_sanitize(self, text: str) -> str:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        out = text
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        out = out.replace("!", ".")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        safe = out
        safe = re.sub(r"\b—Ç\.–¥\.", "—Ç_–¥", safe)
        safe = re.sub(r"\b—Ç\.–ø\.", "—Ç_–ø", safe)
        
        chunks = re.split(r"(?<=[\.\?\!])\s+|\n", safe)
        sentences: List[str] = []
        for ch in chunks:
            s = ch.strip()
            if s:
                sentences.append(s)
        
        seen = set()
        deduped: List[str] = []
        for s in sentences:
            key = re.sub(r"\s+", " ", s.lower()).strip()
            if key and key not in seen:
                seen.add(key)
                deduped.append(s)
        
        # –°–∫–ª–µ–π–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        text_out = " ".join(deduped)
        text_out = text_out.replace("—Ç_–¥", "—Ç.–¥.").replace("—Ç_–ø", "—Ç.–ø.")
        text_out = "\n".join(line.rstrip() for line in text_out.splitlines())
        text_out = re.sub(r"\n{3,}", "\n\n", text_out)
        return text_out.strip()
    
    def _sanitize_style(self, text: str) -> str:
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""
        return self._final_sanitize(text)
    
    def _inject_offer(self, response: str, offer: dict) -> str:
        """–û—Ä–≥–∞–Ω–∏—á–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –∫–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞
        
        Args:
            response: –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
            offer: –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –∏–∑ offers_catalog
            
        Returns:
            –û—Ç–≤–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
        """
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å
        response_trimmed = response.rstrip()
        if response_trimmed.endswith('.'):
            response_trimmed = response_trimmed[:-1]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç placement
        if offer.get("placement") == "end_with_urgency":
            transition = "!\n\n"  # –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ –¥–ª—è urgency
        else:
            transition = ".\n\n"  # –û–±—ã—á–Ω–∞—è —Ç–æ—á–∫–∞
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        return f"{response_trimmed}{transition}{offer['text']}"
